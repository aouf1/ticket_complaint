{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12990484,"sourceType":"datasetVersion","datasetId":8222404}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\nimport pandas as pd\nimport csv\nfrom sklearn.preprocessing import LabelEncoder\nimport torch\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport numpy as np\n! pip install evaluate\nimport evaluate\nimport torch\nfrom sklearn.model_selection import train_test_split\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T15:27:35.389943Z","iopub.execute_input":"2025-09-08T15:27:35.390180Z","iopub.status.idle":"2025-09-08T15:27:47.940267Z","shell.execute_reply.started":"2025-09-08T15:27:35.390163Z","shell.execute_reply":"2025-09-08T15:27:47.939551Z"}},"outputs":[{"name":"stderr","text":"2025-09-08 15:27:41.462173: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1757345261.484475     245 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1757345261.491236     245 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.5)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.4)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.13)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.6.15)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# Model name on Hugging Face Hub\nmodel_name = \"bert-base-multilingual-cased\"\n\n# Download tokenizer + weights\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name,num_labels=5)\n\nprint(\"âœ… mBERT downloaded and ready!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T15:27:47.942485Z","iopub.execute_input":"2025-09-08T15:27:47.944012Z","iopub.status.idle":"2025-09-08T15:27:49.016438Z","shell.execute_reply.started":"2025-09-08T15:27:47.943986Z","shell.execute_reply":"2025-09-08T15:27:49.015822Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"âœ… mBERT downloaded and ready!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"path = \"/kaggle/input/complaints/Dataset complaint Management System (1).csv\"\n\ndf = pd.read_csv(\n    path,\n    encoding=\"cp1256\",\n    sep=\",\",                  # change if your delimiter is not comma\n    engine=\"python\",\n    quoting=csv.QUOTE_NONE,   # don't treat quotes specially\n    escapechar=\"\\\\\",          # allow \\\" inside fields\n    on_bad_lines=\"skip\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T15:27:49.017048Z","iopub.execute_input":"2025-09-08T15:27:49.017255Z","iopub.status.idle":"2025-09-08T15:27:49.039680Z","shell.execute_reply.started":"2025-09-08T15:27:49.017237Z","shell.execute_reply":"2025-09-08T15:27:49.038956Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\n\npath = r\"/kaggle/input/complaints/Dataset complaint Management System (1).csv\"\n\ndef load_any_table(path):\n    # Peek first few bytes to detect XLSX/ZIP\n    with open(path, \"rb\") as f:\n        sig = f.read(4)\n    is_zip = sig == b\"PK\\x03\\x04\"  # XLSX is a zip\n\n    if is_zip:\n        # It's actually an Excel file with a wrong extension\n        return pd.read_excel(path, engine=\"openpyxl\")\n    else:\n        # It's a real CSV\n        for enc in [\"utf-8-sig\", \"cp1256\", \"ISO-8859-1\"]:\n            try:\n                return pd.read_csv(\n                    path,\n                    encoding=enc,\n                    sep=\",\",\n                    engine=\"python\",\n                    quotechar='\"',\n                    doublequote=True,\n                    escapechar=\"\\\\\",\n                    on_bad_lines=\"skip\",\n                    skipinitialspace=True,\n                )\n            except Exception:\n                continue\n        raise RuntimeError(\"Could not parse as CSV with common encodings.\")\n\ndf = load_any_table(path)\nTEXT_COL = \"ComplaintText\"   # <-- change to exact name\nLABEL_COL = \"Category\"        # <-- change to exact name\n\n# Keep only those two\ndf = df[[TEXT_COL, LABEL_COL]].dropna().reset_index(drop=True)\n\n# Rename to standard names\ndf = df.rename(columns={TEXT_COL: \"text\", LABEL_COL: \"labels\"})\n\nprint(df.shape)\nprint(df.head(5))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T15:27:49.040559Z","iopub.execute_input":"2025-09-08T15:27:49.040827Z","iopub.status.idle":"2025-09-08T15:27:49.322267Z","shell.execute_reply.started":"2025-09-08T15:27:49.040800Z","shell.execute_reply":"2025-09-08T15:27:49.321458Z"}},"outputs":[{"name":"stdout","text":"(4061, 2)\n                                                text                  labels\n0  Access to certain buildings is restricted with...    Facilities_Logistics\n1         Access to supplementary videos is blocked.       Coursers_Training\n2  Administrative emails are not replied to promp...  Certificates_Documents\n3  Administrative office does not respond to urge...  Certificates_Documents\n4  Administrative office hours are not updated on...  Certificates_Documents\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"\n\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n\n# Apply tokenization\ntokenized_datasets = df['text'].apply(lambda x: tokenize_function({'text': x}))\n\n# Adding tokenized columns to the DataFrame\ndf['input_ids'] = tokenized_datasets.apply(lambda x: x['input_ids'])\ndf['attention_mask'] = tokenized_datasets.apply(lambda x: x['attention_mask'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T15:27:49.323597Z","iopub.execute_input":"2025-09-08T15:27:49.324499Z","iopub.status.idle":"2025-09-08T15:27:50.355926Z","shell.execute_reply.started":"2025-09-08T15:27:49.324475Z","shell.execute_reply":"2025-09-08T15:27:50.355338Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\n# Label encoding the labels\nlabel_encoder = LabelEncoder()\ndf['labels'] = label_encoder.fit_transform(df['labels'])\n\n# Show the label encoding\nprint(label_encoder.classes_)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T15:27:50.356724Z","iopub.execute_input":"2025-09-08T15:27:50.356950Z","iopub.status.idle":"2025-09-08T15:27:50.363554Z","shell.execute_reply.started":"2025-09-08T15:27:50.356932Z","shell.execute_reply":"2025-09-08T15:27:50.362873Z"}},"outputs":[{"name":"stdout","text":"['Certificates_Documents' 'Coursers_Training' 'Facilities_Logistics'\n 'Finance_Admin' 'IT_Support ']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T15:27:50.364281Z","iopub.execute_input":"2025-09-08T15:27:50.364643Z","iopub.status.idle":"2025-09-08T15:27:50.379076Z","shell.execute_reply.started":"2025-09-08T15:27:50.364618Z","shell.execute_reply":"2025-09-08T15:27:50.378466Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport torch\nfrom datasets import Dataset\n# Split the data into train (80%), validation (10%), and test (10%)\ntrain_df, temp_df = train_test_split(df, test_size=0.2, random_state=42)  # 80% train, 20% temp (for validation and test)\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # Split temp into 50% validation and 50% test\n\n# Move tensors to GPU if device is CUDA\nfrom transformers import Trainer, TrainingArguments, AutoTokenizer, DataCollatorWithPadding\nfrom datasets import Dataset\nimport torch\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\n\n# Function to tokenize the datasets\ndef tokenize_function(examples):\n    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n\n# Create Dataset from pandas dataframe\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)\n\n# Apply tokenization to the datasets (returns dictionaries)\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)\n\n# Move tensors to the device (GPU or CPU)\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Function to move data to the correct device after tokenization\ndef move_to_device(dataset):\n    def move_example(example):\n        # Only move tensors (input_ids, attention_mask, labels) to device\n        return {k: torch.tensor(v).to(device) if isinstance(v, (list, torch.Tensor)) else v for k, v in example.items()}\n    return dataset.map(move_example, batched=False)\n\n# Move datasets to device (GPU or CPU)\ntrain_dataset = move_to_device(train_dataset)\nval_dataset = move_to_device(val_dataset)\ntest_dataset = move_to_device(test_dataset)\n\nprint(f\"Training dataset size: {len(train_dataset)}\")\nprint(f\"Validation dataset size: {len(val_dataset)}\")\nprint(f\"Test dataset size: {len(test_dataset)}\")\n\n# Ensure the model is on the correct device (GPU or CPU)\nmodel = model.to(device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T15:27:50.380801Z","iopub.execute_input":"2025-09-08T15:27:50.381333Z","iopub.status.idle":"2025-09-08T15:27:59.088349Z","shell.execute_reply.started":"2025-09-08T15:27:50.381314Z","shell.execute_reply":"2025-09-08T15:27:59.087767Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3248 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"455e579b7a094835acbf24d86210d93d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/406 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de943e624d5941ebb17f84eea8763766"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/407 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20ca035521954969a078331c7734922d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3248 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5cf9cc316bf496eb9cf272c3fa7e019"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/406 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62cd7d1bf418453881f6eb23b6063e07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/407 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dc9a8b0edf264569b9be4dca5930a837"}},"metadata":{}},{"name":"stdout","text":"Training dataset size: 3248\nValidation dataset size: 406\nTest dataset size: 407\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from transformers import DataCollatorWithPadding\n\n# Create a data collator to batch data correctly\nddata_collator = DataCollatorWithPadding(tokenizer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T15:27:59.089052Z","iopub.execute_input":"2025-09-08T15:27:59.089290Z","iopub.status.idle":"2025-09-08T15:27:59.093062Z","shell.execute_reply.started":"2025-09-08T15:27:59.089272Z","shell.execute_reply":"2025-09-08T15:27:59.092339Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"for param in model.bert.encoder.parameters():\n    param.requires_grad = False\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T15:27:59.093781Z","iopub.execute_input":"2025-09-08T15:27:59.094019Z","iopub.status.idle":"2025-09-08T15:27:59.109912Z","shell.execute_reply.started":"2025-09-08T15:27:59.093994Z","shell.execute_reply":"2025-09-08T15:27:59.109241Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# Count total and trainable parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"ðŸ”¢ Total parameters: {total_params:,}\")\nprint(f\"ðŸŸ¢ Trainable parameters: {trainable_params:,}\")\nprint(f\"ðŸŸ¡ Frozen parameters: {total_params - trainable_params:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T15:27:59.110643Z","iopub.execute_input":"2025-09-08T15:27:59.111424Z","iopub.status.idle":"2025-09-08T15:27:59.124997Z","shell.execute_reply.started":"2025-09-08T15:27:59.111389Z","shell.execute_reply":"2025-09-08T15:27:59.124382Z"}},"outputs":[{"name":"stdout","text":"ðŸ”¢ Total parameters: 177,857,285\nðŸŸ¢ Trainable parameters: 92,802,821\nðŸŸ¡ Frozen parameters: 85,054,464\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"!pip uninstall -y wandb\n\nfrom transformers import Trainer, TrainingArguments\n\n# Define the training arguments\n# Load the accuracy metric\naccuracy_metric = evaluate.load(\"accuracy\")\n\n# Define the compute_metrics function\ndef compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    preds = predictions.argmax(axis=1)  # Convert logits to class predictions\n    return accuracy_metric.compute(predictions=preds, references=labels)\n\n# Define the training arguments\ntraining_args = TrainingArguments(\n    output_dir='./results',             # Directory to save results and checkpoints\n    num_train_epochs=50,                 # Number of training epochs\n    per_device_train_batch_size=16,     # Batch size for training\n    per_device_eval_batch_size=16,      # Batch size for evaluation\n    warmup_steps=500,                   # Number of warmup steps for learning rate scheduler\n    weight_decay=0.01,                  # Strength of weight decay\n    logging_dir='./logs',               # Directory for storing logs\n    logging_steps=10,\n               # Save model after each epoch\n       # Load the best model at the end of training\n    metric_for_best_model=\"accuracy\",  # Use accuracy for evaluating the best model\n    report_to=None                      # Disable WandB logging\n)\n\n# Initialize the Trainer with the model and datasets\ntrainer = Trainer(\n    model=model,                         # The model to be trained\n    args=training_args,                  # Training arguments\n    train_dataset=train_dataset,         # Training dataset\n    eval_dataset=val_dataset,            # Validation dataset\n    compute_metrics=compute_metrics,     # Use the custom compute_metrics function\n    tokenizer=tokenizer,                 # Pass the tokenizer\n    data_collator=ddata_collator         # Use the data collator for padding and tokenization\n)\n\n# Train the model\ntrainer.train()\n\n# Save the trained model\ntrainer.save_model(\"./final_model\")\n\n# Evaluate the model on the validation dataset\neval_results = trainer.evaluate(val_dataset)\nprint(f\"Validation results: {eval_results}\")\n\n# Evaluate the model on the test dataset\ntest_results = trainer.evaluate(test_dataset)\nprint(f\"Test results: {test_results}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-08T15:27:59.125861Z","iopub.execute_input":"2025-09-08T15:27:59.126319Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"\u001b[33mWARNING: Skipping wandb as it is not installed.\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_245/42611968.py:32: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='81' max='10150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [   81/10150 01:03 < 2:14:09, 1.25 it/s, Epoch 0.39/50]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.625600</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>1.628500</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>1.605800</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>1.636300</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>1.611000</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>1.614000</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>1.598700</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"train_results = trainer.evaluate(train_dataset)\nprint(f\"train results: {train_results}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}